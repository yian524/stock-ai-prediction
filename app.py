import streamlit as st
import yfinance as yf
import pandas as pd
import numpy as np
import requests
from bs4 import BeautifulSoup
from snownlp import SnowNLP
import plotly.graph_objs as go
from sklearn.preprocessing import MinMaxScaler
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Dropout, Input
import os
import streamlit as st


# === ğŸ” å¯†ç¢¼ä¿è­·åŠŸèƒ½é–‹å§‹ ===
def check_password():
    """å›å‚³ True ä»£è¡¨ç™»å…¥æˆåŠŸï¼ŒFalse ä»£è¡¨å¤±æ•—"""
    
    # 1. å¦‚æœå·²ç¶“ç™»å…¥æˆåŠŸéï¼Œå°±ç›´æ¥æ”¾è¡Œ
    if st.session_state.get("password_correct", False):
        return True

    # 2. é¡¯ç¤ºè¼¸å…¥æ¡†
    st.header("ğŸ”’ è«‹è¼¸å…¥å­˜å–å¯†ç¢¼")
    password_input = st.text_input("Password", type="password")
    
    if st.button("ç™»å…¥"):
        # é€™è£¡æª¢æŸ¥å¯†ç¢¼æ˜¯å¦ç­‰æ–¼æˆ‘å€‘è¨­å®šçš„ "my_friend_password"
        # (ç¨å¾Œæœƒåœ¨ Secrets è¨­å®šçœŸæ­£çš„å¯†ç¢¼)
        if password_input == st.secrets["PASSWORD"]:
            st.session_state["password_correct"] = True
            st.rerun() # é‡æ–°æ•´ç†é é¢ä»¥é¡¯ç¤ºå…§å®¹
        else:
            st.error("âŒ å¯†ç¢¼éŒ¯èª¤ï¼Œè«‹é‡æ–°è¼¸å…¥")
            
    return False

# å¦‚æœå¯†ç¢¼æª¢æŸ¥æ²’é€šéï¼Œå°±ç›´æ¥åœæ­¢åŸ·è¡Œä¸‹é¢çš„ç¨‹å¼
if not check_password():
    st.stop()

os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'

# --- è¨­å®šç¶²é åŸºæœ¬è³‡è¨Š ---
st.set_page_config(page_title="å°è‚¡ AI é æ¸¬ (å¤šæ¨¡å‹å¹³å‡ç‰ˆ)", layout="wide")

st.title("ğŸ‡¹ğŸ‡¼ å°è‚¡ AI é æ¸¬ç³»çµ± (é›†æˆå­¸ç¿’ç‰ˆ)")
st.markdown("""
æ­¤ç‰ˆæœ¬æ¡ç”¨ **é›†æˆå­¸ç¿’ (Ensemble Learning)** æ¦‚å¿µï¼š
é€éå¤šæ¬¡è¨“ç·´æ¨¡å‹ä¸¦å– **å¹³å‡å€¼**ï¼Œæ¶ˆé™¤å–®æ¬¡è¨“ç·´çš„éš¨æ©Ÿèª¤å·®ï¼Œæä¾›æ›´ç©©å®šçš„é æ¸¬çµæœã€‚
""")

# --- å´é‚Šæ¬„è¨­å®š ---
st.sidebar.header("è¨­å®šåƒæ•¸")

stock_map = {
    "2330 å°ç©é›»": "2330.TW",
    "2317 é´»æµ·": "2317.TW",
    "2454 è¯ç™¼ç§‘": "2454.TW",
    "2603 é•·æ¦®": "2603.TW",
    "3231 ç·¯å‰µ": "3231.TW",
    "2382 å»£é”": "2382.TW",
    "3008 å¤§ç«‹å…‰": "3008.TW",
    "è‡ªè¨‚è¼¸å…¥": "CUSTOM"
}

selected_label = st.sidebar.selectbox("é¸æ“‡è‚¡ç¥¨", list(stock_map.keys()))

if selected_label == "è‡ªè¨‚è¼¸å…¥":
    stock_ticker = st.sidebar.text_input("è«‹è¼¸å…¥å°è‚¡ä»£ç¢¼ (éœ€åŠ  .TW)", "2330.TW")
    stock_id = stock_ticker.split(".")[0] 
    stock_name_for_ptt = stock_id 
else:
    stock_ticker = stock_map[selected_label]
    stock_id = stock_ticker.split(".")[0] 
    stock_name_for_ptt = selected_label.split(" ")[1] 

look_back = st.sidebar.slider("åƒè€ƒéå»å¹¾å¤© (Time Steps)", 10, 60, 30)
epochs = st.sidebar.slider("è¨“ç·´æ¬¡æ•¸ (Epochs)", 1, 30, 10)

# === æ–°å¢ï¼šè®“ä½¿ç”¨è€…æ±ºå®šè·‘å¹¾æ¬¡ ===
ensemble_runs = st.sidebar.slider("é æ¸¬å¹³å‡æ¬¡æ•¸ (å»ºè­° 3~5 æ¬¡)", 1, 10, 3)
st.sidebar.caption(f"æ³¨æ„ï¼šè¨­å®š {ensemble_runs} æ¬¡ï¼Œè¨“ç·´æ™‚é–“å°±æœƒè®Šæˆ {ensemble_runs} å€ã€‚")

# --- çˆ¬èŸ²å‡½å¼ ---

def get_yahoo_news_sentiment(stock_id):
    url = f"https://tw.stock.yahoo.com/quote/{stock_id}.TW/news"
    headers = {'User-Agent': 'Mozilla/5.0'}
    try:
        response = requests.get(url, headers=headers, timeout=10)
        soup = BeautifulSoup(response.text, 'html.parser')
        headlines = soup.find_all('h3')
        scores, titles_list, seen_titles = [], [], set()
        
        count = 0
        for h in headlines:
            if count >= 8: break
            text = h.get_text().strip()
            if len(text) < 5 or text in seen_titles: continue
            seen_titles.add(text)
            s = SnowNLP(text)
            scores.append(s.sentiments)
            titles_list.append(f"[Yahoo] ({s.sentiments:.2f}) {text}")
            count += 1
            
        return (np.mean(scores), titles_list) if scores else (0.5, ["Yahoo: æœªæŠ“å–åˆ°æ–°è"])
    except Exception as e:
        return 0.5, [f"Yahoo éŒ¯èª¤: {e}"]

def get_ptt_sentiment(keyword):
    url = f"https://www.ptt.cc/bbs/Stock/search?q={keyword}"
    headers = {'User-Agent': 'Mozilla/5.0'}
    cookies = {'over18': '1'}
    try:
        response = requests.get(url, headers=headers, cookies=cookies, timeout=10)
        soup = BeautifulSoup(response.text, 'html.parser')
        titles_tags = soup.find_all('div', class_='title')
        scores, titles_list, seen_titles = [], [], set()
        
        count = 0
        for t in titles_tags:
            if count >= 5: break
            if t.find('a'):
                text = t.find('a').get_text().strip()
                if "å·²è¢«åˆªé™¤" in text or text in seen_titles: continue
                seen_titles.add(text)
                s = SnowNLP(text)
                scores.append(s.sentiments)
                titles_list.append(f"[PTT] ({s.sentiments:.2f}) {text}")
                count += 1
        
        return (np.mean(scores), titles_list) if scores else (0.5, ["PTT: ç„¡çµæœ"])
    except Exception as e:
        return 0.5, [f"PTT éŒ¯èª¤: {e}"]

# --- è³‡æ–™è™•ç† ---

def preprocess_data(df, look_back):
    dataset = df['Close'].values.reshape(-1, 1)
    np.random.seed(42) # é€™è£¡å›ºå®šæ˜¯ç‚ºäº†è®“"éå»çš„å‡ç‰¹å¾µ"ä¸€è‡´ï¼Œä¸å½±éŸ¿æ¨¡å‹è¨“ç·´çš„éš¨æ©Ÿæ€§
    sentiment_history = np.random.uniform(0, 1, size=(len(dataset), 1))
    combined_data = np.hstack((dataset, sentiment_history))
    scaler = MinMaxScaler(feature_range=(0, 1))
    scaled_data = scaler.fit_transform(combined_data)
    return scaled_data, scaler, dataset

def build_model(input_shape):
    model = Sequential()
    model.add(Input(shape=input_shape))
    model.add(LSTM(50, return_sequences=True))
    model.add(Dropout(0.2))
    model.add(LSTM(50, return_sequences=False))
    model.add(Dropout(0.2))
    model.add(Dense(25))
    model.add(Dense(1))
    model.compile(optimizer='adam', loss='mean_squared_error')
    return model

# --- ä¸»ç¨‹å¼ ---

st.subheader(f"ğŸ“Š åˆ†ææ¨™çš„ï¼š{stock_ticker}")

with st.spinner('æ­£åœ¨ä¸‹è¼‰æ‰€æœ‰æ­·å²è³‡æ–™ (å¯èƒ½éœ€è¦å¹¾ç§’é˜)...'):
    # âœ… ä¿®æ”¹ 1: æ”¹æˆ "max" æŠ“å–è©²è‚¡ç¥¨ä¸Šå¸‚ä»¥ä¾†çš„æ‰€æœ‰è³‡æ–™
    df = yf.download(stock_ticker, period="max")

if df is not None and not df.empty:
    
    # è³‡æ–™æ¸…æ´— (é˜²å‘†æ©Ÿåˆ¶)
    if isinstance(df.columns, pd.MultiIndex):
        try:
            df = df.xs('Close', axis=1, level=0, drop_level=True)
        except KeyError:
            df = df.iloc[:, 3].to_frame()
    if isinstance(df, pd.Series):
        df = df.to_frame()
    df.columns = ['Close']
    df = df.dropna()

    # âœ… ä¿®æ”¹ 2: æ”¹ç”¨ Plotly ç¹ªè£½å°ˆæ¥­äº’å‹•åœ–è¡¨
    # é€™æœƒç”¢ç”Ÿä¸€å€‹å¯ä»¥ç¸®æ”¾ã€æœ‰æ»‘æ¡¿çš„ K ç·šåœ–æ•ˆæœ
    fig = go.Figure()
    fig.add_trace(go.Scatter(x=df.index, y=df['Close'], mode='lines', name='æ”¶ç›¤åƒ¹'))
    fig.update_layout(
        title=f"{stock_ticker} æ­·å²è‚¡åƒ¹èµ°å‹¢",
        xaxis_title="æ—¥æœŸ",
        yaxis_title="è‚¡åƒ¹",
        xaxis_rangeslider_visible=True, # é–‹å•Ÿä¸‹æ–¹çš„æ™‚é–“æ‹‰æ¡¿
        height=500
    )
    st.plotly_chart(fig, use_container_width=True)
    
    # æº–å‚™è¨“ç·´è³‡æ–™
    # âœ… å„ªåŒ–: é›–ç„¶åœ–è¡¨ç§€ 30 å¹´ï¼Œä½†è¨“ç·´ AI å¦‚æœç”¨ 30 å¹´æœƒè·‘å¤ªä¹…
    # æˆ‘å€‘å– "æœ€è¿‘ 5 å¹´" (ç´„ 1250 å¤©) ä¾†è¨“ç·´å°±å¥½ï¼Œé€™æ¨£æº–ç¢ºåº¦å¤ ï¼Œé€Ÿåº¦ä¹Ÿå¿«
    training_limit = 1250 
    if len(df) > training_limit:
        df_for_training = df.iloc[-training_limit:]
    else:
        df_for_training = df
        
    scaled_data, scaler, raw_data = preprocess_data(df_for_training, look_back)
    
    # ... (ä¸‹æ–¹çš„è¨“ç·´èˆ‡é æ¸¬ç¨‹å¼ç¢¼ä¿æŒä¸è®Š) ...
    # æ³¨æ„ï¼šä¸‹é¢çš„ raw_data è®Šæ•¸æ˜¯ä¾†è‡ª df_for_training
    
    train_size = int(len(scaled_data) * 0.9)
    train_data = scaled_data[0:train_size, :]
    
    x_train, y_train = [], []
    for i in range(look_back, len(train_data)):
        x_train.append(train_data[i-look_back:i, :])
        y_train.append(train_data[i, 0])
    x_train, y_train = np.array(x_train), np.array(y_train)
    
    if st.button(f'ğŸš€ å•Ÿå‹•å¤šæ¨¡å‹åˆ†æ (å…±åŸ·è¡Œ {ensemble_runs} æ¬¡)'):
        
        # 1. çˆ¬èŸ² (åªåšä¸€æ¬¡ï¼Œç¯€çœæ™‚é–“)
        st.write("---")
        st.info("æ­£åœ¨é€²è¡Œæ–°èèˆ‡è¼¿æƒ…åˆ†æ...")
        yahoo_score, yahoo_titles = get_yahoo_news_sentiment(stock_id)
        ptt_score, ptt_titles = get_ptt_sentiment(stock_name_for_ptt)
        if "ç„¡çµæœ" in ptt_titles[0]: ptt_score, ptt_titles = get_ptt_sentiment(stock_id)
        
        final_sentiment = (yahoo_score + ptt_score) / 2
        
        col1, col2 = st.columns(2)
        col1.metric("ç¶œåˆæƒ…ç·’åˆ†æ•¸", f"{final_sentiment:.2f}")
        with col2.expander("æŸ¥çœ‹æ–°èä¾†æº"):
            for t in yahoo_titles + ptt_titles: st.write(t)
            
        # 2. å¤šæ¬¡è¨“ç·´èˆ‡é æ¸¬ (Ensemble)
        st.write("---")
        st.subheader(f"ğŸ§  æ­£åœ¨è¨“ç·´ {ensemble_runs} å€‹ AI æ¨¡å‹...")
        
        progress_bar = st.progress(0)
        status_text = st.empty()
        
        prediction_list = []
        
        # æº–å‚™æœ€å¾Œä¸€å¤©çš„è¼¸å…¥è³‡æ–™
        last_days = scaled_data[-look_back:].copy()
        last_days[-1, 1] = final_sentiment
        X_input = last_days.reshape(1, look_back, 2)
        
        # === è¿´åœˆé–‹å§‹ ===
        for i in range(ensemble_runs):
            status_text.text(f"æ­£åœ¨è¨“ç·´ç¬¬ {i+1} / {ensemble_runs} å€‹æ¨¡å‹...")
            
            # æ¯æ¬¡å»ºç«‹æ–°æ¨¡å‹ï¼Œæ¬Šé‡éƒ½æœƒéš¨æ©Ÿåˆå§‹åŒ–
            model = build_model((x_train.shape[1], x_train.shape[2]))
            
            # è¨“ç·´ (verbose=0 ä¸é¡¯ç¤ºå€‹åˆ¥é€²åº¦ï¼Œä»¥å…æ´—ç‰ˆ)
            model.fit(x_train, y_train, batch_size=16, epochs=epochs, verbose=0)
            
            # é æ¸¬
            pred_scaled = model.predict(X_input, verbose=0)
            
            # åæ­£è¦åŒ–
            temp = np.zeros((1, 2))
            temp[0, 0] = pred_scaled[0, 0]
            pred_price = scaler.inverse_transform(temp)[0][0]
            
            prediction_list.append(pred_price)
            
            # æ›´æ–°é€²åº¦æ¢
            progress_bar.progress((i + 1) / ensemble_runs)
            
        # === è¿´åœˆçµæŸ ===
        
        status_text.text("æ‰€æœ‰æ¨¡å‹è¨“ç·´å®Œæˆï¼")
        
        # è¨ˆç®—çµ±è¨ˆæ•¸æ“š
        avg_price = np.mean(prediction_list)
        max_price = np.max(prediction_list)
        min_price = np.min(prediction_list)
        last_close = raw_data[-1][0]
        
        st.subheader("ğŸ”® æœ€çµ‚é›†æˆé æ¸¬çµæœ")
        
        r_col1, r_col2, r_col3 = st.columns(3)
        r_col1.metric("æ˜¨æ—¥æ”¶ç›¤åƒ¹", f"{last_close:.2f}")
        r_col2.metric("AI å¹³å‡é æ¸¬åƒ¹", f"{avg_price:.2f}", delta=f"{avg_price - last_close:.2f}")
        r_col3.metric("é æ¸¬å€é–“ (æœ€é«˜/æœ€ä½)", f"{max_price:.1f} ~ {min_price:.1f}")
        
        st.write(f"å€‹åˆ¥æ¨¡å‹é æ¸¬å€¼ï¼š {[round(p, 1) for p in prediction_list]}")
        
        if final_sentiment > 0.6 and (avg_price - last_close) > 0:
            st.success("çµè«–ï¼šå¤šæ¨¡å‹ä¸€è‡´çœ‹å¥½ï¼Œæƒ…ç·’æ¨‚è§€ ğŸš€")
        elif final_sentiment < 0.4 and (avg_price - last_close) < 0:
            st.error("çµè«–ï¼šå¤šæ¨¡å‹ä¸€è‡´çœ‹è·Œï¼Œæƒ…ç·’ä¿å®ˆ ğŸ“‰")
        else:
            st.info("çµè«–ï¼šæ¨¡å‹æ„è¦‹åˆ†æ­§æˆ–èˆ‡æƒ…ç·’é¢ä¸ä¸€è‡´ï¼Œå»ºè­°å€é–“æ“ä½œ âš–ï¸")

else:
    st.error("ç„¡æ³•å–å¾—è³‡æ–™ã€‚")